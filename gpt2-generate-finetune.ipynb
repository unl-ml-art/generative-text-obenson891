{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7LoMj4GA4n_"
   },
   "source": [
    "#  GPT-2 Generation and Fine-Tuning\n",
    "\n",
    "This notebook explores GPT-2 (Generative Pretrained Transformer-2) from OpenAI. Read more about it [here](https://openai.com/blog/better-language-models/).\n",
    "\n",
    "Activities include:\n",
    "\n",
    "0. Setup\n",
    "1. Generate samples from pre-trained gpt-2 model\n",
    "2. Fine-tune gpt-2 on text of your choosing. \n",
    "\n",
    "Adapted by Robert Twomey (rtwomey@unl.edu) for Machine Learning for the Arts SP22 from this [google colab](https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce) by [Max Woolf](http://minimaxir.com). See his repo [gpt-2-simple](https://github.com/minimaxir/gpt-2-simple)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run once to install the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q gpt-2-simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "restart the kernel and run the imports\n",
    "\n",
    "RUN THIS EACH TIME\n",
    "\n",
    "# 0.5 Imports: run every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "KBkpRgBCBS2_"
   },
   "outputs": [],
   "source": [
    "import gpt_2_simple as gpt2\n",
    "import tensorflow as tf\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bj2IJLHP3KwE"
   },
   "source": [
    "## GPU\n",
    "\n",
    "Colaboratory uses either a Nvidia T4 GPU or an Nvidia K80 GPU. The T4 is slightly faster than the old K80 for training GPT-2, and has more memory allowing you to train the larger GPT-2 models and generate more text.\n",
    "\n",
    "You can verify which GPU is active by running the cell below.\n",
    "\n",
    "DONT NEED TO RUN THIS EACH TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sUmTooTW3osf",
    "outputId": "c9fcfa4f-277d-4b3e-8974-373066dc157b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Feb 15 11:01:24 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.63.01    Driver Version: 470.63.01    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  On   | 00000000:D8:00.0 Off |                    0 |\n",
      "| N/A   38C    P0    26W / 250W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note the memory usage (0MiB / 32510MiB) for the Tesla V100.\n",
    "You can re-rerun the above cell to see what memory your code/models are using during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0wXB05bPDYxS"
   },
   "source": [
    "## Downloading GPT-2\n",
    "\n",
    "If you're retraining a model on new text, you need to download the GPT-2 model first. \n",
    "\n",
    "There are four released sizes of GPT-2:\n",
    "\n",
    "* `124M` (default): the \"small\" model, 500MB on disk.\n",
    "* `355M`: the \"medium\" model, 1.5GB on disk.\n",
    "* `774M`: the \"large\" model, cannot currently be finetuned with Colaboratory but can be used to generate text from the pretrained model (see later in Notebook)\n",
    "* `1558M`: the \"extra large\", true model. Will not work if a K80/P4 GPU is attached to the notebook. (like `774M`, it cannot be finetuned).\n",
    "\n",
    "Larger models have more knowledge, but take longer to finetune and longer to generate text. You can specify which base model to use by changing `model_name` in the cells below.\n",
    "\n",
    "The next cell downloads it from Google Cloud Storage and saves it in the the current working directory at `/models/<model_name>`.\n",
    "\n",
    "RUN THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P8wSlgXoDPCR",
    "outputId": "10fc0d7c-d18f-4e11-a2af-bfade8b537eb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"355M\" # largest model we can fine-tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run once to download the file\n",
    "\n",
    "don't need to do this again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching checkpoint: 1.05Mit [00:00, 959Mit/s]                                                      \n",
      "Fetching encoder.json: 1.05Mit [00:00, 4.76Mit/s]                                                   \n",
      "Fetching hparams.json: 1.05Mit [00:00, 1.68Git/s]                                                   \n",
      "Fetching model.ckpt.data-00000-of-00001: 1.42Git [00:23, 61.2Mit/s]                                 \n",
      "Fetching model.ckpt.index: 1.05Mit [00:00, 1.15Git/s]                                               \n",
      "Fetching model.ckpt.meta: 1.05Mit [00:00, 4.22Mit/s]                                                \n",
      "Fetching vocab.bpe: 1.05Mit [00:00, 6.20Mit/s]                                                      \n"
     ]
    }
   ],
   "source": [
    "gpt2.download_gpt2(model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQAN3M6RT7Kj"
   },
   "source": [
    "# 1. Generate Text From The Pretrained Model\n",
    "\n",
    "If you want to generate text from the pretrained model pass `model_name` to `gpt2.load_gpt2()` and `gpt2.generate()`. (This is currently the only way to generate text from the 774M or 1558M models with this notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "BAe4NpKNUj2C",
    "outputId": "b09bfe1d-2ff8-4b8a-fffb-273d28d5d4ae",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-15 11:01:31.613578: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-15 11:01:32.255160: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30979 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:d8:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model models/355M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/355M/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "sess = gpt2.start_tf_sess()\n",
    "\n",
    "gpt2.load_gpt2(sess, model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample from the model\n",
    "The follow cell samples from gpt-2, using the provided prefix (seed) and other parameters. It starts the TF session and generates the samples.\n",
    "\n",
    "Try changing the parameters below to change the output: \n",
    "- `prefix` is the prompt. This will be the starting string/seed for your generation. Use your own text. \n",
    "- `temperature` sets the variability/randomness of the output. Range 0.0-1.0\n",
    "- `length` sets the lenght of output (in tokens). max is 1024.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 797
    },
    "id": "-xInIZKaU104",
    "outputId": "56348e28-7d08-45e3-c859-f26c0efd066d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My cat's name is _________ and she's been a member of the ___________ for about three years.\n",
      "\n",
      "I was looking through your website and found a link to your blog. I'm sure you know what I'm talking about. You have a cat. She's been a member of the ___________ for about three years. She's very sweet and her name is _________. She's a great cat. I love her dearly and I love to pet her. She's very loyal. She has\n",
      "====================\n",
      "My cat's name is ____. She's a girl. She's about 10 years old. She's very sweet and sweet-natured. She's always happy, and she's very funny. She likes to play with me. I always say, \"She's a very sweet girl.\"\n",
      "\n",
      "She's a big girl, but she's also a little girl. She's about three or four. She's a little boy. She's about two or three years old. She's a little girl. She's\n",
      "====================\n",
      "My cat's name is ____ and I'm an ____.\"\n",
      "\n",
      "The following text messages were sent to the website:\n",
      "\n",
      "\"It's so good to meet you again. I'm just wondering how you feel about the way I look at you. You're a nice guy, so I can't deny that I would not want to be with you. But I don't really care about it. I'm going to keep on living my life as I do and I'll never have to worry about the way you\n",
      "====================\n",
      "My cat's name is _____.\"\n",
      "\n",
      "I'm not sure what happened, but I decided to go through the post and make sure it was a mistake.\n",
      "\n",
      "As it turns out, the cat's name is _____.\n",
      "\n",
      "I immediately wrote back to her and asked if she knew what I was talking about.\n",
      "\n",
      "She said she thought it was funny and told me that she would be sure to post it on Reddit so that it could be removed.\n",
      "\n",
      "I'm not sure what happened, but\n",
      "====================\n",
      "My cat's name is _____, and he loves me for being the best cat I know. He loves me for the things I do for him, not because he wants to do them. He's not the type of person who is nice to animals, and he will not hurt or bully any of them, but he will help you if you need it. He's always there for me, and I don't need to worry about him being mean or getting angry at me because he's just happy to be around me.\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "gpt2.generate(sess,\n",
    "              model_name=model_name,\n",
    "              prefix=\"My cat's name is \" ,\n",
    "              length=100,\n",
    "              temperature=0.7,\n",
    "              top_p=0.9,\n",
    "              nsamples=5,\n",
    "              batch_size=5\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activities\n",
    "- try varying the prefix. \n",
    "  - what length of prefix works best with the given model? \n",
    "  - how does the choice of prefix change the format/form of the output.\n",
    "- try varying the temperature.\n",
    "- try loading the different sized models (124M, 355M, 774M, 1558M) and generate text without changing the other parameters. \n",
    "  - Do you notice any qualitative differences in the output? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Fine-Tuning GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have already generated with gpt2, you need to reset the tf graph and gpt2 session. Otherwise, we create a new one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aeXshJM-Cuaf",
    "outputId": "a3c75caa-917b-4818-ca2d-d78610d8b6f2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-15 11:01:51.885437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30979 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:d8:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "#model_name = \"355M\" # same model as selected above\n",
    "\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# check if sess exists (e.g. if we ran section 1 above)\n",
    "var_exists = 'sess' in locals() or 'sess' in globals()\n",
    "\n",
    "if not var_exists:\n",
    "    sess = gpt2.start_tf_sess()\n",
    "else:\n",
    "    sess = gpt2.reset_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HeeSKtNWUedE"
   },
   "source": [
    "## Upload a text file\n",
    "For this, we will use a text file you provide to finetune (continue training) GPT-2. You can use any plain text (.txt) file. \n",
    "\n",
    "Simply drag and dropy our text file into the file browser at left. \n",
    "\n",
    "Once you have uploaded your file, update the file name in the cell below, then run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6OFnPCLADfll"
   },
   "outputs": [],
   "source": [
    "file_name = \"cats.txt\" # your file here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LdpZQXknFNY3"
   },
   "source": [
    "## Run the finetuning\n",
    "\n",
    "The next cell will start the actual finetuning of GPT-2. It creates a persistent TensorFlow session which stores the training config, then runs the training for the specified number of `steps`. (to have the finetuning run indefinitely, set `steps = -1`)\n",
    "\n",
    "The model checkpoints will be saved in `/checkpoint/run1` by default. The checkpoints are saved every `save_every` steps (can be changed) and when the cell is stopped.\n",
    "\n",
    "The training might time out after 4ish hours; make sure you end training and save the results so you don't lose them. If your input text is smaller, training might proceed more quickly.\n",
    "\n",
    "Other optional-but-helpful parameters for `gpt2.finetune`:\n",
    "\n",
    "*  **`restore_from`**: Set to `fresh` to start training from the base GPT-2, or set to `latest` to restart training from an existing checkpoint.\n",
    "* **`sample_every`**: Number of steps to print example output\n",
    "* **`print_every`**: Number of steps to print training progress.\n",
    "* **`learning_rate`**:  Learning rate for the training. (default `1e-4`, can lower to `1e-5` if you have <1MB input data)\n",
    "*  **`run_name`**: subfolder within `checkpoint` to save the model. This is useful if you want to work with multiple models (will also need to specify  `run_name` when loading the model)\n",
    "* **`overwrite`**: Set to `True` if you want to continue finetuning an existing model (w/ `restore_from='latest'`) without creating duplicate copies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For larger models, the recommended finetune() parameters are:\n",
      "\tuse_memory_saving_gradients = True\n",
      "\tonly_train_transformer_layers = True\n",
      "\taccumulate_gradients = 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-15 11:02:03.932660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30979 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:d8:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint models/355M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/355M/model.ckpt\n",
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 2676.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset has 9182 tokens\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 | 25.75] loss=2.44 avg=2.44\n",
      "[20 | 45.77] loss=2.42 avg=2.43\n",
      "[30 | 65.80] loss=2.15 avg=2.33\n",
      "[40 | 85.85] loss=2.00 avg=2.25\n",
      "[50 | 105.91] loss=1.69 avg=2.14\n",
      "======== SAMPLE 1 ========\n",
      " got a load of this girl on her first time, and it was an absolute blast, she's such a sweetheart! She's petite, and her body is just incredible too…I can't wait to see how big she gets!\n",
      "\n",
      "This girl is actually a gorgeous redhead with a sexy face, good weight, and a mature figure! She's a sweet girl who just wants love and attention, she really needs everyone to love and respect her and take pride in her, she deserves it!\n",
      "\n",
      "We are big fans of this beautiful girl, she had some super short hairs growing out of her cutie mark a couple of months back but she was so petite her lady-parts were too long for her big boobies, they looked so cute and squishy and round and round! These beautiful cuties were removed a short time ago, they can look so cute but when they are cut they become SO much more…disgusting! We are so sorry these girls lost so many beautiful members we don't take them very seriously and always ask if they can be re-homed!\n",
      "\n",
      "This cute little girl loves taking care of her people, she just had her cutie mark removed and she's so very confused as to what just happened. She's so scared she may get in trouble, if she sees another adult this cutie could be in a terrible situation!\n",
      "\n",
      "This little cutie has an amazing nose and big round eyes, her nose is big and round too, she loves nibbling on her thumb and holding it in her mouth to love it with! Her big pink tongue is super fun to see and she is super affectionate when it's time to play and she gags on her toys for the camera!\n",
      "\n",
      "This cutie has a very pretty face and a pretty body, she knows how to make a good first impression and she is doing that as a cutie! She's very shy and reserved, she will never go out in public…but when she's ready she will run up and say \"hi\" to the stranger! She's extremely funny and sweet, if you have a real cutie like her, you can only imagine how sweet she is!\n",
      "\n",
      "This cutie has a sweet and innocent face, she is very shy around young girls, but she did play with her friends and they were so impressed with her smile! She really has a lot of energy, she has a playful snort when she's excited, and she will roll in her bed to purr! She loves to play with toys, she even likes to scratch people! She's really shy but she's been training herself and she's doing really well! She has an adorable cutie-mark around her neck and she is so curious about her cutie-mark! She will run to you and play for as long as you are there, she loves meeting new people!\n",
      "\n",
      "This adorable girl is very shy, but she makes a great first impression! She loves to play, she likes to tease, she likes attention and she wants love and attention just as much as anything! She likes to play with toys, she likes to play dress up, she loves to tease, and she likes to be touched! She is very friendly and playful, she likes to be pet and cuddle and her cutie-mark is a very pretty pink and it makes her all the sweeter! She's very playful and bubbly, she's funny and silly, she's very sweet, she's gentle and shy, but she's so warm and loving and loving her cutie-mark!\n",
      "\n",
      "Fancy this cutie? She's one shyie, she might be shy around some, but she's not going anywhere! She's all about attention, she wants to be touched, she'd rather be petted, but she says she's too shy to be touched! She gets on all fours, she hops all over the place, she purrs all the time...she would make a perfect pet, and a wonderful companion too!\n",
      "\n",
      "This cutie is very shy, but she's starting to become more open now, she used to be so shy, but she's really coming around now, she's enjoying being pet and having her cutie mark touched, she's just waiting on someone to warmly give it to her! Would she do it herself? She sure as heck would!\n",
      "\n",
      "This shyie girl is so pretty, she came to us with a very very bad case of orange rash, it covered her all over the place and it was so very, very painful. We took her to the vet, they diagnosed her with rashes, they couldn't figure out why it hurt so much, the rashes were very, very common and when we saw the rash, she just loved the pain, she thought it was the end of her day, but she got this great scar on, it was so pretty, it was like it was painted on and it looked like a bird had scratched it and\n",
      "\n",
      "[60 | 139.08] loss=1.62 avg=2.05\n",
      "[70 | 159.13] loss=1.19 avg=1.92\n",
      "[80 | 179.20] loss=1.01 avg=1.80\n",
      "[90 | 199.26] loss=0.72 avg=1.68\n",
      "[100 | 219.34] loss=0.39 avg=1.54\n",
      "Saving checkpoint/run1/model-100\n",
      "======== SAMPLE 1 ========\n",
      " White Girl In Blue is going to be a sexy black chick doing what she does best, sucking dick and getting pounded. She gets ready every morning to let her black titties show for a little while. Once sexy and warm, these beautiful titties become her sweet spot for nasty teen teen teen sex. They're so well lubricated that the second you run your hand over them, they're just as wet as the first time. The fact that she can actually get off to that speaks volumes about how much she likes it when her big black tits are rubbished by some dick. That pretty face of her gets everyone going, and she's got that warm, fuzzy tush that will bring out the best in her fans. Playful and sweet as can be, Rachele is ready to let her black toys go soon...she'll wait outside your door just for you to walk by. Rachele is a sweet girl, and she'll bring lots of attention to your house. She's funny, outgoing, and knows how to get your blood pumping. Rachele won't get us excited, but she will stop me if I'm breezing through the day...she's that type. Rachele has one word answers and is extremely easy to talk to in any language other than her native English. Her favorite movie is The Room, and her two other favorite movies are Toy Story and I, Robot. I had a hard time choosing which cat to purchase, so I went with my \"favorite pet\" – my sis Ashley. Ashley is my all-time favorite cat. We have the same exact coat, the same exact personality, and the same general looks. We're just inseparable. I cannot recommend Rachele enough. I can't believe it took me so long to pick up Rachele! Everything about her is perfect. Her coat is so silver, her fur is so soft, and her eyes are the size of cantaloupes. I can't wait to get her into my loving home and into action immediately…she would do well in a loving home with another loving cat…she'd do well both inside and out…and I can totally imagine her doing just fine in a kitty lounge or something…she's just so sweet…and sweet inside! I picked a cat that I thought I would end up loving for the rest of my life…and boy, has life changed! I just can't believe how much my life has changed in just two months! I was so scared to death that I kept hoping someone would step in and take me away. But no one was near me the whole time…and now that two months have passed, I can honestly say that I am so happy! I was afraid to walk out the door because I was so scared that someone would kill me. But now that two months have passed, I can honestly say that I am so happy! I was afraid to go to the bathroom because I was so scared of walking around naked. But now that I am a naked kitten, I just cry all the time! I used to hate bathroom breaks. Now I get so upset when I see my naked sister getting all dirty! It breaks my heart that I was such a terrible cat that I ended up like this! I was scared of everything! Even a big black cat like Rachele! I used to love running around the house, playing with my cats and dogs, and snuggling with my siblings and cousins. I didn't know it was so late into my foster care! It's been two years since I last saw my sister, but things are looking up for her! She seems to be getting over her fear of the dark and is becoming more outgoing. I know that she will do well in a loving home!\n",
      "\n",
      "Abby\n",
      "Young  Female  Medium  Tabby (Brown / Chocolate), Young  Male  Large  Black, 4'6\", Brown, Tabby (Brown / Chocolate), Birthday September 3, Â 2 Months, Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â #2796<|endoftext|>For more than a year, the White House has been trying to convince a federal judge to strike down Arizona's ban on same-sex marriage. And the administration recently filed a formal motion asking for a temporary restraining order to keep the ban on hold until after the Supreme Court hears arguments in Proposition 8 v. Hodges.\n",
      "\n",
      "But now the White House has reversed course – it's appealing the decision, saying that the high court should strike down Proposition 8. The administration says it now believes that the court should uphold the law.\n",
      "\n",
      "The White House hasn't been a strong advocate of marriage equality in states that haven't approved it. It argues that the issue must be decided on the national stage. But the White House's stance has been widely criticized by business\n",
      "\n",
      "[110 | 254.57] loss=0.27 avg=1.42\n",
      "[120 | 274.64] loss=0.16 avg=1.31\n",
      "[130 | 294.70] loss=0.33 avg=1.23\n",
      "[140 | 314.77] loss=0.12 avg=1.15\n",
      "[150 | 334.84] loss=0.15 avg=1.08\n",
      "Saving checkpoint/run1/model-150\n",
      "WARNING:tensorflow:From /util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages/tensorflow/python/training/saver.py:969: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n"
     ]
    }
   ],
   "source": [
    "gpt2.finetune(sess,\n",
    "              dataset=file_name,\n",
    "              model_name=model_name,\n",
    "              steps=150,            #depending on size of text this changes (maybe 1000?)\n",
    "              restore_from='fresh', # change to 'latest' to resume\n",
    "              run_name='run1',\n",
    "              print_every=10,\n",
    "              learning_rate=1e-5,\n",
    "              #learning rate smaller, learns less per step\n",
    "              sample_every=50,\n",
    "              save_every=100\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXSuTNERaw6K"
   },
   "source": [
    "## Notes on finetuning\n",
    "\n",
    "Keep an eye on the loss, and how quickly it is dropping. A too-rapid drop in loss could be a sign of overfitting, and a learning rate (lr) that is too high. \n",
    "\n",
    "After the model is trained, you can download the checkpoint folder to save your work. Training checkpoints are saved to `checkpoint/run1` (or whatever you chose for the run name above).\n",
    "\n",
    "You can compress it to a rar file and download that. Ask the instructor how.\n",
    "\n",
    "You're done! Feel free to go to the Generate Text From The Trained Model section to generate text based on your retrained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune some more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have already generated with gpt2, you need to reset the tf graph and gpt2 session. Otherwise, we create a new one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aeXshJM-Cuaf",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "a3c75caa-917b-4818-ca2d-d78610d8b6f2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-10 11:18:35.772450: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30979 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:5e:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "#model_name = \"355M\" # same model as selected above\n",
    "\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# check if sess exists (e.g. if we ran section 1 above)\n",
    "var_exists = 'sess' in locals() or 'sess' in globals()\n",
    "\n",
    "if not var_exists:\n",
    "    sess = gpt2.start_tf_sess()\n",
    "else:\n",
    "    sess = gpt2.reset_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fine-tune some more, run the following. Be sure to increase the number of steps (if it was `500` before, change to `1000` to train for 500 more. the number is cumulative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For larger models, the recommended finetune() parameters are:\n",
      "\tuse_memory_saving_gradients = True\n",
      "\tonly_train_transformer_layers = True\n",
      "\taccumulate_gradients = 1\n",
      "\n",
      "Loading checkpoint checkpoint/run1/model-200\n",
      "INFO:tensorflow:Restoring parameters from checkpoint/run1/model-200\n",
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 4369.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset has 4906 tokens\n",
      "Training...\n",
      "Saving checkpoint/run1/model-200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      " a apron; a bed; bedspread; bunk; bunk berth; bunkhouse; bunkhouse; bedroll; blanket; blanket of leaves; blanket; blanket of sea; blanket of ashes; blanket of sun; Christmas; Christmas Day; Christmas Eve; Christmas Eve Party; China; Christmas goose; Christmas greeting card; Christmas sweater; Christmas, New Zealand; Christmas tree; costume; dinner gown; dinner dress; dinner suit; dishdasha; dishroast; dishwasher; dishwashing machine; dung beetle; dance; Earth; English bulldog; English bulldog mix; English bulldog mix bred for; English bulldog mix/breed; English bulldog/dutch mix; English bulldog/German mix; English bulldog/American Bulldog mix; fire ant; football; First World War; French bulldog; German shepherd; hamster; hiker (biking in the dark); hodgkin's lymphoma; hound; Inuit; Japanese knothead steelhead; lunch; malarial fever; Maypole Day; Mount Everest; pig's lung; plover; poochie; ranch house; samurai; squirrel; snow; spoon; spanish bulldog mix; stinking cobra; tea party; tuna; toilet; treadmill\n",
      "\n",
      "A\n",
      "\n",
      "backbencher (someone outside the prime minister's staff); ballroom dancer; B\n",
      "\n",
      "bite of the month; baking soda; birthday; birthday cake\n",
      "\n",
      "baby\n",
      "\n",
      "bicycle; cake; carpet; deodorant; deodorant patch\n",
      "\n",
      "disinfectant; machine; new pair of underwear; new pair of shoes\n",
      "\n",
      "news\n",
      "\n",
      "radio; shirt; tv; washing machine\n",
      "\n",
      "book\n",
      "\n",
      "bicycle\n",
      "\n",
      "clothes hanger; change of clothes; new button-down shirt; fleece jacket\n",
      "\n",
      "golf club; hat; hospital stays\n",
      "\n",
      "in the mood for a date?\n",
      "\n",
      "new pair of running shoes\n",
      "\n",
      "overnight stay\n",
      "\n",
      "pile of laundry; rope\n",
      "\n",
      "spring cleaning\n",
      "\n",
      "walk in closet\n",
      "\n",
      "women's clothing\n",
      "\n",
      "women's style\n",
      "\n",
      "women's period\n",
      "\n",
      "women's clothing for men\n",
      "\n",
      "winterized clothes\n",
      "\n",
      "x amount; zone\n",
      "\n",
      "Y\n",
      "\n",
      "zodiac sign\n",
      "\n",
      "Zorro\n",
      "\n",
      "What will you wear to your first date?\n",
      "\n",
      "beret; blue jeans; chinos\n",
      "\n",
      "comfortable shirt and tie; dress slacks\n",
      "\n",
      "exotic suit\n",
      "\n",
      "fossil tee\n",
      "\n",
      "golf bag\n",
      "\n",
      "jacket\n",
      "\n",
      "maroon shorts\n",
      "\n",
      "plain tee\n",
      "\n",
      "soled hoodie\n",
      "\n",
      "t-shirt\n",
      "\n",
      "velcro\n",
      "\n",
      "video game console\n",
      "\n",
      "crowd pleaser\n",
      "\n",
      "flattering haircut\n",
      "\n",
      "high five\n",
      "\n",
      "keepsake\n",
      "\n",
      "limo\n",
      "\n",
      "plaid shirt\n",
      "\n",
      "dress slacks\n",
      "\n",
      "sandals\n",
      "\n",
      "sunglasses\n",
      "\n",
      "sunglasses acquired at a flea market\n",
      "\n",
      "smart phone\n",
      "\n",
      "suit\n",
      "\n",
      "white shirt with a logo\n",
      "\n",
      "white tie with a star\n",
      "\n",
      "Yoda robes\n",
      "\n",
      "Zone 9 a\n",
      "\n",
      "a\n",
      "\n",
      "agitated\n",
      "\n",
      "accomplice\n",
      "\n",
      "ambassador\n",
      "\n",
      "average\n",
      "\n",
      "animal trainer\n",
      "\n",
      "author\n",
      "\n",
      "airbnb host\n",
      "\n",
      "accountant\n",
      "\n",
      "animal shelter\n",
      "\n",
      "airfare to meet\n",
      "\n",
      "author/joke writer\n",
      "\n",
      "airline\n",
      "\n",
      "animal sanctuary\n",
      "\n",
      "advice columnist\n",
      "\n",
      "batch painter\n",
      "\n",
      "bagel\n",
      "\n",
      "bar\n",
      "\n",
      "baseball\n",
      "\n",
      "best friend\n",
      "\n",
      "beautiful baby daughter\n",
      "\n",
      "bed & breakfast\n",
      "\n",
      "buying a home\n",
      "\n",
      "business travel\n",
      "\n",
      "career\n",
      "\n",
      "checkbook\n",
      "\n",
      "check your money on\n",
      "\n",
      "city center\n",
      "\n",
      "cyber café\n",
      "\n",
      "cyber security\n",
      "\n",
      "company\n",
      "\n",
      "coffee shop\n",
      "\n",
      "cyber security company\n",
      "\n",
      "computer lab\n",
      "\n",
      "club\n",
      "\n",
      "cover band\n",
      "\n",
      "dress rehearsal\n",
      "\n",
      "ecotourism\n",
      "\n",
      "expert\n",
      "\n",
      "fitness enthusiast\n",
      "\n",
      "family member\n",
      "\n",
      "furniture\n",
      "\n",
      "funerals\n",
      "\n",
      "get a new haircut\n",
      "\n",
      "girlfriend\n",
      "\n",
      "golf tournament\n",
      "\n",
      "get a new passport\n",
      "\n",
      "hit the links\n",
      "\n",
      "image editor\n",
      "\n",
      "investigator\n",
      "\n",
      "investigator\n",
      "\n",
      "image processor\n",
      "\n",
      "journalist\n",
      "\n",
      "journalist\n",
      "\n",
      "lawyer\n",
      "\n",
      "linkedin\n",
      "\n",
      "music festival\n",
      "\n",
      "painter\n",
      "\n",
      "personal assistant\n",
      "\n",
      "project manager\n",
      "\n",
      "recycler\n",
      "\n",
      "retiree\n",
      "\n",
      "retirement planning\n",
      "\n",
      "repeat customer\n",
      "\n",
      "reader\n",
      "\n",
      "solar panel\n",
      "\n",
      "skinny latte drinker\n",
      "\n",
      "small business owner\n",
      "\n",
      "spouse\n",
      "\n",
      "stockbroker\n",
      "\n",
      "student loan\n",
      "\n",
      "startup\n",
      "\n",
      "television\n",
      "\n",
      "tax refund\n",
      "\n",
      "vacation\n",
      "\n",
      "walk through\n",
      "\n",
      "white knight\n",
      "\n",
      "zone 10 b\n",
      "\n",
      "background\n",
      "\n",
      "buy a ticket\n",
      "\n",
      "buy a ticket at the opera\n",
      "\n",
      "buy a ticket at a concert\n",
      "\n",
      "buy a ticket at your local art gallery\n",
      "\n",
      "buy a ticket at the movies\n",
      "\n",
      "buy\n",
      "\n",
      "[210 | 42.25] loss=0.03 avg=0.03\n",
      "[220 | 62.08] loss=0.03 avg=0.03\n",
      "[230 | 81.90] loss=0.03 avg=0.03\n",
      "[240 | 101.73] loss=0.02 avg=0.03\n",
      "interrupted\n",
      "Saving checkpoint/run1/model-242\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/gpt_2_simple/gpt_2.py\u001b[0m in \u001b[0;36mfinetune\u001b[0;34m(sess, dataset, steps, model_name, model_dir, combine, batch_size, learning_rate, accumulate_gradients, restore_from, run_name, checkpoint_dir, sample_every, sample_length, sample_num, multi_gpu, save_every, print_every, max_checkpoints, use_memory_saving_gradients, only_train_transformer_layers, optimizer, overwrite, reuse)\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccumulate_gradients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m                     sess.run(\n\u001b[0m\u001b[1;32m    340\u001b[0m                         opt_compute, feed_dict={context: sample_batch()})\n",
      "\u001b[0;32m/util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[1;32m    968\u001b[0m                          run_metadata_ptr)\n",
      "\u001b[0;32m/util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1190\u001b[0;31m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0m\u001b[1;32m   1191\u001b[0m                              feed_dict_tensor, options, run_metadata)\n",
      "\u001b[0;32m/util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1367\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1368\u001b[0;31m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0m\u001b[1;32m   1369\u001b[0m                            run_metadata)\n",
      "\u001b[0;32m/util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1374\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1359\u001b[0;31m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0m\u001b[1;32m   1360\u001b[0m                                       target_list, run_metadata)\n",
      "\u001b[0;32m/util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1450\u001b[0m                           run_metadata):\n\u001b[0;32m-> 1451\u001b[0;31m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0m\u001b[1;32m   1452\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_986549/3782541432.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m gpt2.finetune(sess,\n\u001b[0m\u001b[1;32m      2\u001b[0m               \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m               \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m               \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m               \u001b[0mrestore_from\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latest'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# change to 'latest' to resume\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/gpt_2_simple/gpt_2.py\u001b[0m in \u001b[0;36mfinetune\u001b[0;34m(sess, dataset, steps, model_name, model_dir, combine, batch_size, learning_rate, accumulate_gradients, restore_from, run_name, checkpoint_dir, sample_every, sample_length, sample_num, multi_gpu, save_every, print_every, max_checkpoints, use_memory_saving_gradients, only_train_transformer_layers, optimizer, overwrite, reuse)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'interrupted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/gpt_2_simple/gpt_2.py\u001b[0m in \u001b[0;36msave\u001b[0;34m()\u001b[0m\n\u001b[1;32m    281\u001b[0m             os.path.join(checkpoint_path,\n\u001b[1;32m    282\u001b[0m                          'model-{}').format(counter-1))\n\u001b[0;32m--> 283\u001b[0;31m         saver.save(\n\u001b[0m\u001b[1;32m    284\u001b[0m             \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs, save_debug_info)\u001b[0m\n\u001b[1;32m   1186\u001b[0m           \u001b[0mmodel_checkpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m           model_checkpoint_path = sess.run(\n\u001b[0m\u001b[1;32m   1189\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m               {self.saver_def.filename_tensor_name: checkpoint_file})\n",
      "\u001b[0;32m/util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[1;32m    968\u001b[0m                          run_metadata_ptr)\n\u001b[1;32m    969\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[0;31m# or if the call is a partial run that specifies feeds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1190\u001b[0;31m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0m\u001b[1;32m   1191\u001b[0m                              feed_dict_tensor, options, run_metadata)\n\u001b[1;32m   1192\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1368\u001b[0;31m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0m\u001b[1;32m   1369\u001b[0m                            run_metadata)\n\u001b[1;32m   1370\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1373\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1359\u001b[0;31m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0m\u001b[1;32m   1360\u001b[0m                                       target_list, run_metadata)\n\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1449\u001b[0m   def _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list,\n\u001b[1;32m   1450\u001b[0m                           run_metadata):\n\u001b[0;32m-> 1451\u001b[0;31m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0m\u001b[1;32m   1452\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m                                             run_metadata)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gpt2.finetune(sess,\n",
    "              dataset=file_name,\n",
    "              model_name=model_name,\n",
    "              steps=20,\n",
    "              restore_from='latest', # change to 'latest' to resume\n",
    "              run_name='run1',\n",
    "              print_every=10,\n",
    "              learning_rate=1e-5,\n",
    "              sample_every=100,\n",
    "              save_every=100\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClJwpF_ACONp"
   },
   "source": [
    "# 3. Generate Text From The Finetuned Model\n",
    "\n",
    "After you've trained the model or loaded a retrained model from checkpoint, you can now generate text. `generate` generates a single text from the loaded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4RNY6RBI9LmL",
    "outputId": "82574eaa-d39a-4665-b611-e5172848da57",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nigel Farage has criticised the Government over its decision to cut off Leicestershire Council's funding.\n",
      "\n",
      "Mr Farage told the BBC's Sunday Politics the cuts will have a \"huge effect\" on the community.\n",
      "\n",
      "He said: \"We're in a very difficult position.\n",
      "\n",
      "\"We've had a real good relationship with the council but now from the beginning of next year the amount we get will be cut by 40%.\n",
      "\n",
      "\"We don't know what the consequences are and it's very difficult for the local people who live in the area.\n",
      "\n",
      "\"We will almost certainly lose our council tax-free status by the end of next year.\"\n",
      "\n",
      "We don't know what the consequences are and it's very difficult for the local people who live in the area Nigel Farage\n",
      "Leicestershire Council\n",
      "The council said it had received this funding immediately after the election.\n",
      "\n",
      "Chief executive Mark Jones said: \"We are very concerned by the Government's decision to cut funding to Leicestershire Council immediately after the election.\n",
      "\n",
      "\"We have been in close touch with Leicestershire Council over the last few months to discuss the implications of this decision, and we have now been made aware that funding will be halted immediately.\n",
      "\n",
      "\"We will continue to work with the Government and the opposition parties to reach a deal that respects local interests and is acceptable to all.\"\n",
      "\n",
      "A Department for Communities and Local Government spokesperson said: \"We are delighted to be able to say that the budget we released today will help cash-strapped councils to keep their schools open and offer the best possible education to their children.\n",
      "\n",
      "\"Our new schools funding formula means that local authorities will get more money to spend, while school places remain open for as long as possible.\n",
      "\n",
      "\"We know that when it comes to schools, people are in charge. They sent a clear message to Theresa May that they want to keep their schools open and offer the best possible service to their children.\n",
      "\n",
      "\"We will continue to work with the local authorities to find the best way to deliver this funding across the country.\"\n",
      "\n",
      "The Liberal Democrats have already said they will not be backing the school funding deal.\n",
      "\n",
      "Leader Vince Cable said: \"This is yet more government blokeery. It means more schools will close, more children will be left behind, and more council tax-payers will end up paying for the Liberal Democrats' nonsense.\"<|endoftext|>This article is about a/an in the Power Rangers franchise.\n",
      "\n",
      "Supreme Master Gatsby (ラフェンター・マスター, Raifu Mātsā?) is a Super Sentai and Super Ninja Turtle created by Bandai. He is the main antagonist of the first season of the Power Rangers franchise, voiced by John Barrowman.\n",
      "\n",
      "Contents show]\n",
      "\n",
      "History\n",
      "\n",
      "Gatsby was a well-known mobster who was executed for his part in the 1998 murder of his business partner and business rival, Edgar \"Ed\" Mendoza. His remains were exhumed and found to be that of a young boy, who had been brutally murdered. After a full forensic examination, the results proved that the body of Gatsby had been brutally mutilated and mutilated to the point of complete impenence, and that the remains had been stored in a cold, dark and unsanitary location for over a year. The cause of death was asphyxiation, as the cause of death was determined to have been asphyxiation.\n",
      "\n",
      "Gatsby made a full and frank confession to investigators, and was then transported to the morgue to undergo a full forensic examination. Upon examination, it was determined that the cause of death was asphyxiation, and that the remains had been stored in a cold, dark and unsanitary location for over a year. The cause of death was determined to have been asphyxiation, and that the remains had been stored in a cold, dark and unsanitary location for over a year. After the autopsy, the cause of death was determined to have been asphyxiation.\n",
      "\n",
      "Gatsby's remains were then transported to a morgue where they were placed in a freezer until their cause of death could be determined. On March 14, 2007, the cause of death was determined to have been asphyxiation.\n",
      "\n",
      "Gatsby's remains were then placed in the custody of the City of Lancaster.\n",
      "\n",
      "Powers and Abilities\n",
      "\n",
      "As a Super Mutant, Gatsby has the superhuman strength, speed, and agility of a 6-year-old boy. He is also an expert marksman, possessing a matchless array of hunting and camping weapons.\n",
      "\n",
      "He is extremely loyal to his family, and will even sacrifice his life to protect them.\n",
      "\n",
      "History\n",
      "\n",
      "Gatsby was a well-known mobster who was executed for his part in the 1998 murder of his business partner, Edgar \"Ed\"\n"
     ]
    }
   ],
   "source": [
    "gpt2.generate(sess, run_name='run1') # no prefix, unconditional generation, tells you what it has learned without prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baby baby. You're so cute when you're happy.\n",
      "\n",
      "Your smile is the best thing to ever happen to my soul.\n",
      "\n",
      "You are my best friend.\n",
      "\n",
      "You will always be in my heart.\n",
      "\n",
      "I will love and care for you like I've never loved you before.\n",
      "\n",
      "You will always be in my prayers.\n",
      "\n",
      "You will always be in my thoughts.\n",
      "I will always be in my prayers.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "I will forever be in your prayers.\n",
      "\n",
      "I will forever be in your prayers.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "I will forever be in your prayers.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "I will forever be in your prayers.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "You will forever be in my thoughts.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt2.generate(sess, run_name='run1', prefix=\"Baby\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oF4-PqF0Fl7R"
   },
   "source": [
    "## Notes\n",
    "If you're creating an API based on your model and need to pass the generated text elsewhere, you can do `text = gpt2.generate(sess, return_as_list=True)[0]`\n",
    "\n",
    "You can also pass in a `prefix` to the generate function to force the text to start with a given character sequence and generate text from there (good if you add an indicator when the text starts).\n",
    "\n",
    "You can also generate multiple texts at a time by specifing `nsamples`. Unique to GPT-2, you can pass a `batch_size` to generate multiple samples in parallel, giving a massive speedup (in Colaboratory, set a maximum of 20 for `batch_size`).\n",
    "\n",
    "Other optional-but-helpful parameters for `gpt2.generate` and friends:\n",
    "\n",
    "*  **`length`**: Number of tokens to generate (default 1023, the maximum)\n",
    "* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
    "* **`top_k`**: Limits the generated guesses to the top *k* guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set `top_k=40`)\n",
    "* **`top_p`**: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with `top_p=0.9`)\n",
    "* **`truncate`**: Truncates the input text until a given sequence, excluding that sequence (e.g. if `truncate='<|endoftext|>'`, the returned text will include everything before the first `<|endoftext|>`). It may be useful to combine this with a smaller `length` if the input texts are short.\n",
    "*  **`include_prefix`**: If using `truncate` and `include_prefix=False`, the specified `prefix` will not be included in the returned text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjjEN2Tafhl2"
   },
   "source": [
    "For bulk generation, you can generate a large amount of text to a file and sort out the samples locally on your computer. The next cell will generate a generated text file with a unique timestamp.\n",
    "\n",
    "You can rerun the cells as many times as you want for even more generated texts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TobyDaniels\n",
      "\n",
      "SpongeBob SquarePants\n",
      "Tom Khaos\n",
      "This guy is an absolute beast. He is almost 6 years old and weighs about 100 pounds. He is very affectionate and super playful. He likes to sit on your lap and snuggle against you, or you can lean into him and watch him go berserk in pure bliss. He is super playful and energetic and will have an instant fan-base. He will be a huge hit on the bridge!\n",
      "\n",
      "Kiki\n",
      "Scout\n",
      "SpongeBob SquarePants\n",
      "Kiki is a golden retriever mix that was born in Orange County, CA. She was the favorite of the kennels when she was a young kitten, and still is today. She is a sweet, affectionate gal, with a lot of energy. She loves to chase her toys, and to sit down and enjoy long, quiet naps with humans. She is well-mannered and tidy, and would do well in many homes.\n",
      "\n",
      "Salty\n",
      "English Bulldog\n",
      "SpongeBob SquarePants\n",
      "This is a gorgeous little guy! Salty is a very affectionate bulldog mix. He is very easy-going around people, and\n",
      "====================\n",
      "Toby, the cat, was kidnapped recently along with his sister, Violet. They have yet to be found. Help us find Toby and Violet so they can have the best start in their lives. They will truly make a difference in their forever homes!*Pets are not allowed in the home. They must be on a per pet basis.\n",
      "\n",
      "New to the Area?\n",
      "We're just minutes from the Allegheny Riverfront, the University of Pittsburgh and of course, the Steel City! In the heart of Pittsburgh, just minutes from downtown, the University of Pittsburgh, the Center for Arts and Science, the Carnegie Science Museum, the Science Center and around two hours from Yorktown Heights, Easton, and the Allegheny River, are just some of the exciting things to do and see in the Lehigh Valley. The area offers a variety of outdoor activities for all ages, from hiking trails and playgrounds to bird watching and boat races, to picnicking and cricket. Pets are not allowed in the Lehigh Valley. Residents and visitors are welcome to play with and visit their furry friends, but pets must be on a leash at all times.\n",
      "\n",
      "Kitten Friendly\n",
      "We are a 1-year-old, spayed female\n",
      "====================\n",
      "Toby is a big boy and loves to play with dolls. He is very playful and likes to make other animals laugh. He is good with a camera and loves to share his photos on instagram and twitter. He is a big boy and loves attention. He will not appreciate you trying to play with him.\n",
      "\n",
      "Koral\n",
      "Koral is a 1 year old male kitty cat, good with children and other pets. He is friendly, affectionate and loving towards all cats and humans. He is not over-protective or clingy but would be fine with most. He likes to sit and relax and relax. He will get his own spot but be reasonable with that one. He loves his cat-sitting area and will cuddle with the people that take him in. He is clean and tidy when he's not fussing or napping. He is eligible for FIV/Killer Chow and a Shot for KA.\n",
      "\n",
      "Koral the Calico Collie is a fine-tuned breed of Calico Terrier belonging to Kathryn & Ryan. Kathryn is a very affectionate girl that loves to be petted and cuddled. Her love for people is obvious and she adores people eyes. Her playful nature\n",
      "====================\n",
      "Toby, he's my best friend. I just got him from a shelter when I was pregnant with my second. He was such atscue he would chase & play with a feather until I got him. A month later when I showed up at the shelter with a burned paw he was gone. I went to the vet & he said \"he may have had his tail pulled\" So I did some research online & found this guy! Bought him a t-shirt with his picture & he LOVES it. He's ready to make a new forever home with these hiker friends!\n",
      "\n",
      "Koral, he's my first dog & he's doing GREAT! He's sat with his tail wagged & he followed me around the house doing silly things like sit on the step & play with a pole. He's been so relaxed & content he made me pull the cat door open so he could use the cat door w/o having to ask! He's been so happy since & he's been playing so much less hee hee hee. He's been busy eating cat treats, napping & drinking water since he got there & he's very happy!\n",
      "\n",
      "Koral, he's my first dog & it's been\n",
      "====================\n",
      "Toby Howey, LHP, Grade A: The fastball is 94.0, curveball is 84.0, changeup is average, slider is average, curveball command is good; there isn't much there that isn't already there; there's some mechanical wear on the slider that could limit his command, but there's no serious change in how he's going to play; if he can get over the mid-90s, he could be a very solid starter; if he has to sit out a month or two to get to the low-90s range, that could be a problem; the thing about Howey is that he always seems to be getting better; the issue is, the more he gets, the worse he gets; the closer he gets to the low-90s range, the worse it will be; at this point, how much longer can he go without being the best version of himself?\n",
      "\n",
      "Gio Gonzalez, RHP, Grade A-: After a slow start to the season, Gio Gonzalez has picked up the pace; he's since gone from a mid-90s fastball to a low-80s curveball, to a 97-84 mph heater; his changeup is another plus\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "gpt2.generate(sess,\n",
    "              length=250,\n",
    "              temperature=0.7,\n",
    "              prefix=\"Toby\",\n",
    "              nsamples=5,\n",
    "              batch_size=5\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "Fa6p6arifSL0"
   },
   "outputs": [],
   "source": [
    "gen_file = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n",
    "\n",
    "gpt2.generate_to_file(sess,\n",
    "                      destination_path=gen_file,\n",
    "                      length=500,\n",
    "                      temperature=0.7,\n",
    "                      nsamples=100,\n",
    "                      batch_size=20\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-LRex8lfv1g"
   },
   "source": [
    "Download the file by hand in the browser at left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a Trained Model Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTa6zf3e_9gV"
   },
   "source": [
    "Uploaded your saved checkpoint and unzip it.\n",
    "\n",
    "The next cell will allow you to load the retrained model checkpoint + metadata necessary to generate text.\n",
    "\n",
    "This will reset or start the tensorflow session as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-fxL77nvAMAX",
    "outputId": "8938432a-3b86-4102-f32b-362721ecb897"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-10 11:21:42.332368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30979 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:5e:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint checkpoint/run1/model-200\n",
      "INFO:tensorflow:Restoring parameters from checkpoint/run1/model-200\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "if not sess:\n",
    "    sess = gpt2.start_tf_sess()\n",
    "else:\n",
    "    sess = gpt2.reset_session(sess)\n",
    "\n",
    "gpt2.load_gpt2(sess, run_name='run1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ig-KVgkCDCKD"
   },
   "source": [
    "# Etcetera\n",
    "\n",
    "If the notebook has errors (e.g. GPU Sync Fail), force-kill the Colaboratory virtual machine and restart it with the command below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rIHiVP53FnsX"
   },
   "outputs": [],
   "source": [
    "!kill -9 -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmTXWNUygS5E"
   },
   "source": [
    "# License\n",
    "\n",
    "MIT License\n",
    "\n",
    "Copyright (c) 2019 Max Woolf\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "- Max's [blog post](https://minimaxir.com/2019/09/howto-gpt2/) for more information how to use this notebook!\n",
    "- Original repo: [gpt-2-simple](https://github.com/minimaxir/gpt-2-simple) by [Max Woolf](http://minimaxir.com). \n",
    "- Original [google colab](https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce) from Max."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Train a GPT-2 Text-Generating Model w/ GPU",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "TensorFlow GPU 2.6 (py39)",
   "language": "python",
   "name": "tensorflow-gpu-2.6-py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
